{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7992a",
   "metadata": {},
   "source": [
    "# Hotel Recognition to Combat Human Trafficking | Train\n",
    "    2021-05-09\n",
    "    Edward Sims\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.00 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf182792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import cv2\n",
    "from datetime import datetime as dt\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "\n",
    "# Model cross validation and evaluation\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Package options\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8795e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs Available: 1\n",
      "REPLICAS: 1\n",
      "Number of CPU Cores: 1\n"
     ]
    }
   ],
   "source": [
    "# Check GPU config\n",
    "print(f\"Number of GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "\n",
    "STRATEGY = tf.distribute.get_strategy()\n",
    "REPLICAS = STRATEGY.num_replicas_in_sync\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "\n",
    "# Data access\n",
    "GPU_OPTIONS = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "\n",
    "# Get number of cpu cores for multiprocessing\n",
    "try:\n",
    "    CPUS = 1#int(multiprocessing.cpu_count() / 2) \n",
    "except NotImplementedError:\n",
    "    CPUS = 1 # Default number of cores\n",
    "    \n",
    "print(f\"Number of CPU Cores: {CPUS}\")\n",
    "\n",
    "# Disable eager execution for mAP metric\n",
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d8c86",
   "metadata": {},
   "source": [
    "## 2.00 Data Preparation\n",
    "### 2.01 Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b90807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir_path         = \"../input/hotel-id-2021-fgvc8\"\n",
    "train_images_dir_path = os.path.join(data_dir_path, \"train_images\")\n",
    "test_images_dir_path  = os.path.join(data_dir_path, \"test_images\")\n",
    "\n",
    "train_metadata_path   = os.path.join(data_dir_path, \"train.csv\")\n",
    "sample_sub_path       = os.path.join(data_dir_path, \"sample_submission.csv\")\n",
    "\n",
    "# Read csv data\n",
    "train_metadata        = pd.read_csv(train_metadata_path, parse_dates=[\"timestamp\"])\n",
    "sample_sub            = pd.read_csv(sample_sub_path)\n",
    "\n",
    "# Remove 2 duplicated records from metadata\n",
    "train_metadata_dupes = train_metadata.loc[train_metadata.groupby(\"image\")[\"image\"].transform(\"count\") > 1, ]\n",
    "train_metadata_dupes_idx = train_metadata_dupes.iloc[[1, 3]].index\n",
    "train_metadata = train_metadata.drop(train_metadata_dupes_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95040f",
   "metadata": {},
   "source": [
    "### 2.02 Set default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fc440d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: baseline_xception_128x128_5folds_seed14\n"
     ]
    }
   ],
   "source": [
    "# Define key parameters\n",
    "SEED = 14\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Default image dimensions\n",
    "ROWS     = 128 # Default row size\n",
    "COLS     = 128 # Default col size\n",
    "CHANNELS = 3\n",
    "\n",
    "# Default modelling parameters\n",
    "EPOCHS     = 100\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE   = 10\n",
    "KFOLDS = 5\n",
    "\n",
    "# Uncomment as appropriate\n",
    "#MODEL_TO_USE = \"densenet121\"\n",
    "#MODEL_TO_USE = \"densenet169\"\n",
    "#MODEL_TO_USE = \"densenet201\"\n",
    "#MODEL_TO_USE = \"efficientnet_b0\"\n",
    "#MODEL_TO_USE = \"efficientnet_b1\"\n",
    "#MODEL_TO_USE = \"efficientnet_b2\"\n",
    "#MODEL_TO_USE = \"efficientnet_b3\"\n",
    "#MODEL_TO_USE = \"efficientnet_b4\"\n",
    "#MODEL_TO_USE = \"efficientnet_b5\"\n",
    "#MODEL_TO_USE = \"inception_resnetv2\"\n",
    "#MODEL_TO_USE = \"inceptionv3\"\n",
    "#MODEL_TO_USE = \"resnet50v2\"\n",
    "#MODEL_TO_USE = \"resnet101v2\"\n",
    "#MODEL_TO_USE = \"resnext50\"\n",
    "#MODEL_TO_USE = \"resnext101\"\n",
    "#MODEL_TO_USE = \"resnet152v2\"\n",
    "#MODEL_TO_USE = \"vgg19\"\n",
    "MODEL_TO_USE = \"xception\"\n",
    "\n",
    "# Initialise dataset for first time or use previously written data\n",
    "INITIALISE_DATA = True\n",
    "\n",
    "# Treat model as baseline or not\n",
    "IS_BASELINE = True\n",
    "\n",
    "if IS_BASELINE == True:\n",
    "    model_name_save = f\"baseline_{MODEL_TO_USE}_{str(ROWS)}x{str(COLS)}_{str(KFOLDS)}folds_seed{str(SEED)}\"\n",
    "elif IS_BASELINE == False:\n",
    "    model_name_save = f\"{MODEL_TO_USE}_{str(ROWS)}x{str(COLS)}_{str(KFOLDS)}folds_seed{str(SEED)}\"\n",
    "\n",
    "# Create models path if does not exist already\n",
    "if not os.path.exists(f\"models/{model_name_save}\"):\n",
    "    os.mkdir(f\"models/{model_name_save}\")\n",
    "\n",
    "print(f\"Model name: {model_name_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e37b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata preparation\n",
    "def get_is_weekend(timestamp_col):\n",
    "    \"\"\"\n",
    "    Returns boolean for whether timestamp is a weekend.\n",
    "    \"\"\"\n",
    "    timestamp_col_weekday = timestamp_col.dt.weekday\n",
    "    # Allocate booleans - Weekends are designated 6 & 7\n",
    "    timestamp_col_weekday = timestamp_col_weekday.apply(lambda x: False if x < 5 else True)\n",
    "    \n",
    "    return timestamp_col_weekday\n",
    "\n",
    "# Extract year, month and hour from timestamp feature\n",
    "train_metadata[\"year\"] = train_metadata[\"timestamp\"].dt.year\n",
    "train_metadata[\"month\"] = train_metadata[\"timestamp\"].dt.month\n",
    "train_metadata[\"hour\"] = train_metadata[\"timestamp\"].dt.hour\n",
    "# Extract is_weekend from timestamp\n",
    "train_metadata[\"is_weekend\"] = get_is_weekend(train_metadata[\"timestamp\"])\n",
    "train_metadata = train_metadata.drop(\"timestamp\", axis=1)\n",
    "# Create full image path feature\n",
    "train_metadata[\"image_path\"] = train_images_dir_path + \"/\" + train_metadata[\"chain\"].astype(\"str\")\n",
    "train_metadata[\"image_path\"] = train_metadata[\"image_path\"] + \"/\" + train_metadata[\"image\"]\n",
    "\n",
    "# Extract labels from metadata\n",
    "y_train_vector = np.array(train_metadata[\"hotel_id\"])\n",
    "\n",
    "# Get all full image paths\n",
    "train_images_path_vector = np.array(train_metadata[\"image_path\"])\n",
    "\n",
    "# Following metadata preparation, get number of classes and groups constants\n",
    "NUM_CLASSES = np.max(y_train_vector) + 1\n",
    "GROUPS = np.array(train_metadata[\"chain\"], train_metadata[\"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9fa5d",
   "metadata": {},
   "source": [
    "### 2.03 Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa8bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, augment=False):\n",
    "    \"\"\"\n",
    "    Read an image from a file, decode it into a dense tensor, and resize.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize(image, [ROWS, COLS])\n",
    "        if augment:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_hue(image, 0.01)\n",
    "            image = tf.image.random_saturation(image, 0.7, 1.3)\n",
    "            image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "            image = tf.image.random_brightness(image, 0.1)\n",
    "        return image\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "def load_all_images(images_paths):\n",
    "    \"\"\"\n",
    "    Read in multiple images asynchrously using load_image() function.\n",
    "    \"\"\"\n",
    "    pool = multiprocessing.Pool(processes=CPUS)\n",
    "    images = pool.map(load_image, images_paths)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc1463f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Writer is closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5608824a98b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         }\n\u001b[1;32m     28\u001b[0m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/tf_record.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \"\"\"\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Writer is closed."
     ]
    }
   ],
   "source": [
    "# Create TFRecords from data if INITIALISE_DATA is True - otherwise skip this step\n",
    "if INITIALISE_DATA == True:\n",
    "    \n",
    "    # Helper functions to make feature definitions more readable\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    \n",
    "    # Load data into a numpy array\n",
    "    \n",
    "    category = train_metadata.loc[train_metadata.image == train_images_path_vector[0].split(\"/\")[-1], \"chain\"]\n",
    "    category = category.item()\n",
    "    \n",
    "    # Create TFRecord filewriter\n",
    "    writer = tf.io.TFRecordWriter(\"../input/tfrecords/test\")\n",
    "\n",
    "    for image_path in train_images_path_vector[0:2]:\n",
    "        image = load_image(image_path)\n",
    "        image_name = image_path.split(\"/\")[-1]\n",
    "        label = train_metadata.loc[train_metadata.image == image_name, \"hotel_id\"].item()\n",
    "        category = train_metadata.loc[train_metadata.image == image_name, \"chain\"].item()\n",
    "        feature = {\n",
    "            \"label\": _int64_feature(label),\n",
    "            \"image\": _bytes_feature(np.array(image).tostring())\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299e7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78affb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1a517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342936fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9355a2",
   "metadata": {},
   "source": [
    "### 2.04 Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_img_augmentations(img, y):\n",
    "    \"\"\"Make augmentations to single train image and copy label accordingly\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : array\n",
    "        Image to augment\n",
    "    y : array\n",
    "        Label array to copy as per number of augmentations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        np.array of original image and augmented images, and their corresponding labels.\n",
    "    \"\"\"\n",
    "    img_augs = np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(img, axis=0),\n",
    "            # Flip left-right\n",
    "            np.expand_dims(np.fliplr(img), axis=0),\n",
    "            # Rotate 90 degrees clockwise\n",
    "            np.expand_dims(np.rot90(img, 1), axis=0),\n",
    "            # Rotate 180 degrees\n",
    "            np.expand_dims(np.rot90(img, 2), axis=0),\n",
    "            # Rotate 270 degrees clockwise\n",
    "            np.expand_dims(np.rot90(img, 3), axis=0)\n",
    "        ),\n",
    "        axis=0\n",
    "    )\n",
    "        \n",
    "    # Copy labels accordingly\n",
    "    y_augs = img_augs.shape[0]\n",
    "    y = np.repeat(y, y_augs)\n",
    "\n",
    "    return img_augs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_img_augmentations(img):\n",
    "    \"\"\"\n",
    "    Returns augmented test images and original for prediction.\n",
    "    \"\"\"\n",
    "    img_augs = np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(img, axis=0),\n",
    "            np.expand_dims(np.rot90(img, 1), axis=0),\n",
    "            np.expand_dims(np.rot90(img, 2), axis=0),\n",
    "            np.expand_dims(np.rot90(img, 3), axis=0),\n",
    "            np.expand_dims(np.fliplr(img), axis=0),\n",
    "            np.expand_dims(np.fliplr(np.rot90(img, 1)), axis=0),\n",
    "            np.expand_dims(np.fliplr(np.rot90(img, 2)), axis=0),\n",
    "            np.expand_dims(np.fliplr(np.rot90(img, 3)), axis=0)),\n",
    "        axis=0\n",
    "    )\n",
    "        \n",
    "    return(img_augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674d633",
   "metadata": {},
   "source": [
    "## 3.00 Modelling\n",
    "### 3.01 Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31899b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start          = 0.00001, \n",
    "               lr_max            = 0.0008, \n",
    "               lr_min            = 0.00001, \n",
    "               lr_rampup_epochs  = 20, \n",
    "               lr_sustain_epochs = 0, \n",
    "               lr_exp_decay      = 0.8):\n",
    "    \n",
    "    lr_max = lr_max * STRATEGY.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "\n",
    "    return lrfn\n",
    "\n",
    "lrfn = build_lrfn()\n",
    "lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "\n",
    "plt.plot([lrfn(epoch) for epoch in range(EPOCHS)])\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc2b06",
   "metadata": {},
   "source": [
    "### 3.02 Compiler Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mean Average Precision at K metric\n",
    "map_at_k = tf.compat.v1.metrics.average_precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df631bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true = np.array([[4], [4], [4], [4], [4]]).astype(np.int64)\n",
    "#y_true = tf.identity(y_true)\n",
    "#\n",
    "#y_pred = np.array([[0.1, 0.3, 0.5, 0.7, 0.9, 0.1, 0.1, 0.2, 0.6],\n",
    "#                   [0.1, 0.3, 0.5, 0.7, 0.9, 0.1, 0.1, 0.2, 0.6],\n",
    "#                   [0.1, 0.3, 0.5, 0.7, 0.9, 0.1, 0.1, 0.2, 0.6],\n",
    "#                   [0.1, 0.3, 0.5, 0.7, 0.9, 0.1, 0.1, 0.2, 0.6],\n",
    "#                   [0.1, 0.3, 0.5, 0.7, 0.9, 0.1, 0.1, 0.2, 0.6]\n",
    "#                   ]).astype(np.float32)\n",
    "#y_pred = tf.identity(y_pred)\n",
    "#\n",
    "#_, m_ap = map_at_k(y_true, y_pred, 5)\n",
    "#\n",
    "#sess = tf.Session()\n",
    "#sess.run(tf.local_variables_initializer())\n",
    "#\n",
    "#stream_vars = [i for i in tf.local_variables()]\n",
    "#\n",
    "#tf_map = sess.run(m_ap)\n",
    "#print(tf_map)\n",
    "#\n",
    "#tmp_rank = tf.nn.top_k(y_pred, 5)\n",
    "#\n",
    "#print(sess.run(tmp_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593ee76",
   "metadata": {},
   "source": [
    "### 3.03 CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73139d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we'll feed the images into before concatenation\n",
    "def get_cnn_model(model_to_use=MODEL_TO_USE):\n",
    "    \"\"\"Get the pretrained CNN model specified.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kfold : int \n",
    "        Fold that the CV is currently on (to determine img size)\n",
    "    model_to_use : str \n",
    "        Model to retrieve\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model_return : tensorflow.python.keras.engine.functional.Functional\n",
    "        A pretrained CNN model without top included.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_shape = (ROWS, COLS, CHANNELS)\n",
    "    \n",
    "    # DenseNet121\n",
    "    if model_to_use == \"densenet121\":\n",
    "        from tensorflow.keras.applications import DenseNet121\n",
    "        return DenseNet121(input_shape=input_shape, include_top=False)\n",
    "        \n",
    "    # DenseNet169\n",
    "    elif model_to_use == \"densenet169\":\n",
    "        from tensorflow.keras.applications import DenseNet169\n",
    "        return DenseNet169(input_shape=input_shape, include_top=False)\n",
    "        \n",
    "    # DenseNet201\n",
    "    elif model_to_use == \"densenet201\":\n",
    "        from tensorflow.keras.applications import DenseNet201\n",
    "        return DenseNet201(input_shape=input_shape, include_top=False)\n",
    "            \n",
    "    # EfficientNet_B0\n",
    "    elif model_to_use == \"efficientnet_b0\":\n",
    "        import efficientnet.tfkeras as efficientnet\n",
    "        return efficientnet.EfficientNetB0(\n",
    "            input_shape=input_shape, include_top=False\n",
    "        )\n",
    "    \n",
    "    # EfficientNet_B1\n",
    "    elif model_to_use == \"efficientnet_b1\":\n",
    "        import efficientnet.tfkeras as efficientnet\n",
    "        return efficientnet.EfficientNetB1(\n",
    "            input_shape=input_shape, include_top=False\n",
    "        )\n",
    "            \n",
    "    # EfficientNet_B2\n",
    "    elif model_to_use == \"efficientnet_b2\":\n",
    "        import efficientnet.tfkeras as efficientnet\n",
    "        return efficientnet.EfficientNetB2(\n",
    "            input_shape=input_shape, include_top=False\n",
    "        )\n",
    "            \n",
    "    # EfficientNet_B3\n",
    "    elif model_to_use == \"efficientnet_b3\":\n",
    "        import efficientnet.tfkeras as efficientnet\n",
    "        return efficientnet.EfficientNetB3(\n",
    "            input_shape=input_shape, include_top=False\n",
    "        )\n",
    "            \n",
    "    # EfficientNet_B4\n",
    "    elif model_to_use == \"efficientnet_b4\":\n",
    "        import efficientnet.tfkeras as efficientnet\n",
    "        return efficientnet.EfficientNetB4(\n",
    "            input_shape=input_shape, include_top=False\n",
    "        )\n",
    "            \n",
    "    # EfficientNet_B5\n",
    "    elif model_to_use == \"efficientnet_b5\":\n",
    "        import efficientnet.tfkeras as efficientnet\n",
    "        return efficientnet.EfficientNetB5(\n",
    "            input_shape=input_shape, include_top=False\n",
    "        )\n",
    "\n",
    "    # InceptionResNetV2\n",
    "    elif model_to_use == \"inception_resnetv2\":\n",
    "        from tensorflow.keras.applications import InceptionResNetV2\n",
    "        return InceptionResNetV2(input_shape=input_shape, include_top=False)\n",
    "\n",
    "    # InceptionV3\n",
    "    elif model_to_use == \"inceptionv3\":\n",
    "        from tensorflow.keras.applications import InceptionV3\n",
    "        return InceptionV3(input_shape=input_shape, include_top=False)\n",
    "    \n",
    "    # NasNetLarge\n",
    "    elif model_to_use == \"nasnetlarge\":\n",
    "        from tensorflow.keras.applications import NASNetLarge\n",
    "        return NASNetLarge(input_shape=input_shape, include_top=False)\n",
    "        \n",
    "    # ResNet50V2\n",
    "    elif model_to_use == \"resnet50v2\":\n",
    "        from tensorflow.keras.applications import ResNet50V2\n",
    "        return ResNet50V2(input_shape=input_shape, include_top=False)\n",
    "\n",
    "    # ResNet101V2\n",
    "    elif model_to_use == \"resnet101v2\":\n",
    "        from tensorflow.keras.applications import ResNet101V2\n",
    "        return ResNet101V2(input_shape=input_shape, include_top=False)\n",
    "\n",
    "    # ResNet152V2\n",
    "    elif model_to_use == \"resnet152v2\":\n",
    "        from tensorflow.keras.applications import ResNet152V2\n",
    "        return ResNet152V2(input_shape=input_shape, include_top=False)\n",
    "\n",
    "    # ResNeXt50\n",
    "    elif model_to_use == \"resnext50\":\n",
    "        from keras_applications.resnext import ResNeXt50\n",
    "        return ResNeXt50(\n",
    "            input_shape=input_shape, \n",
    "            include_top=False, \n",
    "            classes=classes,\n",
    "            backend=keras.backend, \n",
    "            layers=keras.layers, \n",
    "            models=keras.models, \n",
    "            utils=keras.utils\n",
    "        )\n",
    "\n",
    "    # ResNeXt101\n",
    "    elif model_to_use == \"resnext101\":\n",
    "        from keras_applications.resnext import ResNeXt101\n",
    "        return ResNeXt101(\n",
    "            input_shape=input_shape, \n",
    "            include_top=False, \n",
    "            classes=classes,\n",
    "            backend=keras.backend, \n",
    "            layers=keras.layers, \n",
    "            models=keras.models, \n",
    "            utils=keras.utils\n",
    "        )\n",
    "        \n",
    "    # VGG19\n",
    "    elif model_to_use == \"vgg19\":\n",
    "        from tensorflow.keras.applications import VGG19\n",
    "        return VGG19(input_shape=input_shape, include_top=False)\n",
    "\n",
    "    # Xception\n",
    "    elif model_to_use == \"xception\":\n",
    "        from tensorflow.keras.applications import Xception\n",
    "        return Xception(input_shape=input_shape, include_top=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7da111",
   "metadata": {},
   "source": [
    "### 3.04 Create TF Record Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files, augment = False, shuffle = False, repeat = False, \n",
    "                labeled=True, return_image_names=True, batch_size=16, dim=256):\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    ds = ds.cache()\n",
    "    \n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(1024*8)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "        \n",
    "    if labeled: \n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n",
    "                    num_parallel_calls=AUTO)      \n",
    "    \n",
    "    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim), \n",
    "                                               imgname_or_label), \n",
    "                num_parallel_calls=AUTO)\n",
    "    \n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36deba83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generates data for Keras\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, labels, batch_size, dim, n_channels, n_classes, shuffle):\n",
    "        \"Initialization\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one batch of data\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size: (index + 1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples\n",
    "        \"\"\" \n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = load_image(train_images_path_list[int(ID)])\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        # Amend any inconsistent y label dimensions\n",
    "        y = np.append(y, np.expand_dims(np.array(self.n_classes - 1), axis=0), axis=0)\n",
    "\n",
    "        #print(f\"self.n_classes: {self.n_classes}\")\n",
    "        #print(f\"np.max(y): {np.max(y)}\")\n",
    "        return X, keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad838ad6",
   "metadata": {},
   "source": [
    "### 3.05 Define and Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_model(model_cnn=MODEL_TO_USE, verbose=1):\n",
    "    model_cnn = get_cnn_model(model_cnn)\n",
    "    # Add a global spatial average pooling layer\n",
    "    x = model_cnn.output\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    # Define output layer\n",
    "    output = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    # Define final model\n",
    "    model = Model(inputs=model_cnn.input, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2668a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_BASELINE == True:\n",
    "    # Define CV strategy\n",
    "    gkf = GroupKFold(n_splits=KFOLDS)\n",
    "    loss_scores = []\n",
    "\n",
    "    for fold, (tdx, vdx) in enumerate(gkf.split(train_images_path_list[0:1000], y_train_vector[0:1000], groups=GROUPS[0:1000])):\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        # Create name to save model by\n",
    "        model_save_path = f\"models/{model_name_save}/{model_name_save}_{str(fold)}.h5\"\n",
    "        \n",
    "        print(\"\\nGathering data...\")\n",
    "        # Shuffle tdx and vdx\n",
    "        np.random.shuffle(tdx)\n",
    "        np.random.shuffle(vdx)\n",
    "        \n",
    "        # Set parameter dictionary\n",
    "        params = {\n",
    "            \"dim\": (ROWS, COLS),\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"n_classes\": NUM_CLASSES,\n",
    "            \"n_channels\": CHANNELS,\n",
    "            \"shuffle\": True\n",
    "        }\n",
    "        \n",
    "        # Set dictionaries for generator\n",
    "        partition = {\"train\": tdx.astype('str'), \"validation\": vdx.astype('str')}\n",
    "        labels = dict(\n",
    "            zip(\n",
    "                np.concatenate((tdx, vdx)).astype('str'), \n",
    "                list(y_train_vector[np.concatenate((tdx, vdx))])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Define data generators\n",
    "        training_generator = DataGenerator(partition[\"train\"], labels, **params)\n",
    "        validation_generator = DataGenerator(partition[\"validation\"], labels, **params)\n",
    "        \n",
    "        # Get baseline model\n",
    "        print(\"Loading model...\")\n",
    "        model = get_baseline_model()\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")        \n",
    "        \n",
    "        # Define learning rate schedule\n",
    "        lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "        \n",
    "        # Define early stopping parameters\n",
    "        es = EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            mode=\"min\",\n",
    "            restore_best_weights=True, \n",
    "            verbose=0, \n",
    "            patience=PATIENCE\n",
    "        )\n",
    "        \n",
    "        # Define model checkpoint parameters\n",
    "        mc = ModelCheckpoint(\n",
    "            filepath=model_save_path, \n",
    "            save_best_only=True, \n",
    "            save_weights_only=False,\n",
    "            monitor=\"val_loss\", \n",
    "            mode=\"min\",\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "        # Fit model\n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            x=training_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks = [es, lr, mc],\n",
    "            use_multiprocessing=True,\n",
    "            workers=CPUS,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get val_loss for the best model (one saved with ModelCheckpoint)\n",
    "        loss = min(history.history[\"val_loss\"])\n",
    "        print(f\"LOSS: \\t\\t{loss}\")\n",
    "        \n",
    "        #print('MAKING VALIDATION PREDICTIONS...')\n",
    "        \n",
    "        # Load best model\n",
    "        #model = load_model(model_save_name)\n",
    "        # Make validation predictions\n",
    "        #preds = model.predict(X_vdx_best_model)\n",
    "        #\n",
    "        ## Calculate OOF loss \n",
    "        #oof_loss = metric(np.array(y_vdx_best_model), np.array(preds))\n",
    "    \n",
    "        #print('FOLD ' + str(fold) + ' LOSS: ' + str(oof_loss))\n",
    "        #print('--------------------------------------------------------------------------------------------')\n",
    "        #time.sleep(2)\n",
    "        #loss_scores.append(oof_loss)\n",
    "    \n",
    "        ## Clean up\n",
    "        k.clear_session()\n",
    "        #gc.collect()\n",
    "        #os.remove(model_save_name_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc22f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921794f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb3866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50b7ede6",
   "metadata": {},
   "source": [
    "### 3.06 Bayesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caada479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search dimensions\n",
    "dim_learning_rate    = Real(low=1e-4,   high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1,   high=6,    name='num_dense_layers')\n",
    "dim_num_input_nodes  = Integer(low=1,   high=4096, name='num_input_nodes')\n",
    "dim_num_dense_nodes  = Integer(low=1,   high=4096, name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu','leaky_relu','elu','threshold_relu'], name='activation')\n",
    "dim_batch_size       = Integer(low=1,   high=64,   name='batch_size')\n",
    "dim_patience         = Integer(low=3,   high=15,   name='patience')\n",
    "dim_optimiser = Categorical(\n",
    "    categories=['sgd','adam','rms_prop','ada_delta','ada_grad', 'ada_max','n_adam','ftrl'], name='optimiser'\n",
    ")\n",
    "dim_optimiser_decay  = Real(low=1e-6,   high=1e-2, name='optimiser_decay')\n",
    "dim_dropout_layer = Categorical(categories=['dropout','gaussian_dropout','alpha_dropout'],name='dropout_layer')\n",
    "dim_dropout_val      = Real(low=0.1,    high=0.8,  name='dropout_val')\n",
    "\n",
    "dimensions = [\n",
    "    dim_learning_rate,\n",
    "    dim_num_dense_layers,\n",
    "    dim_num_input_nodes,\n",
    "    dim_num_dense_nodes,\n",
    "    dim_activation,\n",
    "    dim_batch_size,\n",
    "    dim_patience,\n",
    "    dim_optimiser,\n",
    "    dim_optimiser_decay,\n",
    "    dim_dropout_layer,\n",
    "    dim_dropout_val,\n",
    "]\n",
    "\n",
    "# Set default hyperparameters\n",
    "default_parameters = [\n",
    "    1e-3,      # learning_rate\n",
    "    1,         # num_dense_layers\n",
    "    512,       # num_input_nodes\n",
    "    16,        # num_dense_nodes\n",
    "    'relu',    # activation\n",
    "    64,        # batch_size\n",
    "    3,         # patience\n",
    "    'adam',    # optimiser\n",
    "    1e-3,      # optimiser_decay\n",
    "    'dropout', # dropout_layer\n",
    "    0.1,       # dropout_val\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693fc97",
   "metadata": {},
   "source": [
    "### 3.07 Train Model with Bayesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5529d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CV strategy\n",
    "kf = KFold(n_splits=KFOLDS, random_state=SEED)\n",
    "loss_scores = []\n",
    "best_params = pd.DataFrame(\n",
    "    columns=['kfold','selected_features','num_features', 'num_components', 'use_embedding', 'seed'])\n",
    "\n",
    "for fold, (tdx, vdx) in enumerate(kf.split(X, y)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    # Create name to save model by\n",
    "    model_save_name = 'models/' + model_name_save + '/' + model_name_save + '_' + str(fold) + '.h5'\n",
    "    model_save_name_temp = 'models/' + model_name_save + '/' + 'TEMP_'+ model_name_save+ '_' + str(fold) + '.h5'\n",
    "    \n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def get_hyperopts(learning_rate, \n",
    "                      num_dense_layers, \n",
    "                      num_input_nodes, \n",
    "                      num_dense_nodes,\n",
    "                      activation, \n",
    "                      batch_size,\n",
    "                      patience,\n",
    "                      optimiser,\n",
    "                      optimiser_decay,\n",
    "                      dropout_layer,\n",
    "                      dropout_val):\n",
    "\n",
    "        # Define key parameters - these are affected by parameter search so must be done inside function\n",
    "        BATCH_SIZE = batch_size\n",
    "        PATIENCE   = patience\n",
    "\n",
    "        \n",
    "        # Fetch in-fold data\n",
    "        X_tdx, X_vdx, y_tdx, y_vdx = X.iloc[tdx, :], X.iloc[vdx, :], y.iloc[tdx, :], y.iloc[vdx, :]\n",
    "\n",
    "        # Define activation layers\n",
    "        if activation == 'relu':\n",
    "            ACTIVATION = ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            ACTIVATION = LeakyReLU()\n",
    "        elif activation == 'elu':\n",
    "            ACTIVATION = ELU()\n",
    "        elif activation == 'threshold_relu':\n",
    "            ACTIVATION = ThresholdedReLU()\n",
    "\n",
    "        # Define regularisation layers\n",
    "        if dropout_layer == 'dropout':\n",
    "            REG_LAYER = Dropout(dropout_val)\n",
    "        elif dropout_layer == 'gaussian_dropout':\n",
    "            REG_LAYER = GaussianDropout(dropout_val)\n",
    "        elif dropout_layer == 'alpha_dropout':\n",
    "            REG_LAYER = AlphaDropout(dropout_val)\n",
    "\n",
    "        # Define optimisers #\n",
    "        if optimiser == 'sgd':\n",
    "            OPTIMISER = SGD(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'adam':\n",
    "            OPTIMISER = RMSprop(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'rms_prop':\n",
    "            OPTIMISER = Adam(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_delta':\n",
    "            OPTIMISER = Adadelta(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_grad':\n",
    "            OPTIMISER = Adagrad(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_max':\n",
    "            OPTIMISER = Adamax(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'n_adam':\n",
    "            OPTIMISER = Nadam(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ftrl':\n",
    "            OPTIMISER = Ftrl(lr=learning_rate, decay=optimiser_decay)\n",
    "\n",
    "        ## BUILD MODEL BASED ON INPUTTED BAYESIAN HYPERPARAMETERS ##\n",
    "        # Input layer #\n",
    "        if USE_EMBEDDING == 1:\n",
    "            inputs = []\n",
    "            embeddings = []\n",
    "            for col in cat_cols:\n",
    "                # Create categorical embedding for each categorical feature\n",
    "                input_ = Input(shape=(1,))\n",
    "                input_dim = int(X_tdx[col].max() + 1)\n",
    "                embedding = Embedding(input_dim=input_dim, output_dim=10, input_length=1)(input_)\n",
    "                embedding = Reshape(target_shape=(10,))(embedding)\n",
    "                inputs.append(input_)\n",
    "                embeddings.append(embedding)\n",
    "            input_numeric = Input(shape=(len(num_cols),))\n",
    "            embedding_numeric = Dense(num_input_nodes)(input_numeric) \n",
    "            embedding_numeric = ACTIVATION(embedding_numeric) \n",
    "            inputs.append(input_numeric)\n",
    "            embeddings.append(embedding_numeric)\n",
    "            x = Concatenate()(embeddings)\n",
    "        if USE_EMBEDDING == 0:\n",
    "            input_ = Input(shape=(X_tdx.shape[1], ))\n",
    "            x = Dense(num_input_nodes)(input_)\n",
    "        # Hidden layers #\n",
    "        for i in range(num_dense_layers):\n",
    "            layer_name = f'layer_dense_{i+1}'\n",
    "            x = Dense(num_dense_nodes, name=layer_name)(x)\n",
    "            x = ACTIVATION(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = REG_LAYER(x) \n",
    "        # Output layer #\n",
    "        output = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "        if USE_EMBEDDING == 1:\n",
    "            model = Model(inputs, output)\n",
    "        elif USE_EMBEDDING == 0:\n",
    "            model = Model(input_, output)\n",
    "\n",
    "        \n",
    "        # COMPILE MODEL #\n",
    "        model.compile(optimizer=OPTIMISER, \n",
    "                      loss='binary_crossentropy')\n",
    "\n",
    "        # Define learning rate schedule\n",
    "        lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "        \n",
    "        # Define early stopping parameters\n",
    "        es = EarlyStopping(monitor='val_loss', \n",
    "                           mode='min',\n",
    "                           restore_best_weights=True, \n",
    "                           verbose=0, \n",
    "                           patience=PATIENCE)\n",
    "        \n",
    "        # Define model checkpoint parameters\n",
    "        mc = ModelCheckpoint(filepath=model_save_name_temp, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False,\n",
    "                             monitor='val_loss', \n",
    "                             mode='min',\n",
    "                             verbose=0)\n",
    "\n",
    "        if USE_EMBEDDING == 1:\n",
    "            # Separate data to fit into embedding and numerical input layers\n",
    "            X_tdx = [np.absolute(X_tdx[i]) for i in cat_cols] + [X_tdx[num_cols]]\n",
    "            X_vdx = [np.absolute(X_vdx[i]) for i in cat_cols] + [X_vdx[num_cols]]\n",
    "\n",
    "        # FIT MODEL #\n",
    "        print('TRAINING...')\n",
    "        history = model.fit(X_tdx, y_tdx,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            callbacks = [es, lr, mc],\n",
    "                            verbose=0,\n",
    "                            validation_split=0.25\n",
    "                           )\n",
    "        \n",
    "        # Get val_loss for the best model (one saved with ModelCheckpoint)\n",
    "        loss = min(history.history['val_loss'])\n",
    "        print(f'CURRENT LOSS: \\t\\t{loss}')\n",
    "        \n",
    "        # Save best loss and parameters to global memory\n",
    "        global best_loss\n",
    "        global best_params\n",
    "\n",
    "        # If the classification loss of the saved model is improved\n",
    "        if loss < best_loss:\n",
    "            model.save(model_save_name)\n",
    "            best_loss = loss\n",
    "            \n",
    "            # Save transformed validation arrays (so they can be used for prediction)\n",
    "            global X_vdx_best_model, y_vdx_best_model\n",
    "            X_vdx_best_model, y_vdx_best_model = X_vdx, y_vdx\n",
    "            \n",
    "            ### SAVE MODEL PARAMETERS ### \n",
    "            best_params = best_params.loc[best_params.kfold != fold]\n",
    "            best_params = best_params.append({'kfold'            : fold,\n",
    "                                              'selected_features': selected_features,\n",
    "                                              'num_features'     : NUM_FEATURES,\n",
    "                                              'num_components'   : NUM_COMPONENTS,\n",
    "                                              'use_embedding'    : USE_EMBEDDING,\n",
    "                                              'seed'             : SEED}, \n",
    "                                             ignore_index=True)\n",
    "            best_params.to_csv('final_classifier_parameters/' + model_name_save + '.csv', index=False)\n",
    "            \n",
    "        print(f'BEST LOSS: \\t\\t{best_loss}\\n')\n",
    "\n",
    "        del model\n",
    "        k.clear_session()\n",
    "        return(loss)\n",
    "    \n",
    "    ## RUN BAYESIAN HYPERPARAMETER SEARCH ##\n",
    "    print('RUNNING PARAMETER SEARCH...\\n')\n",
    "    time.sleep(2)\n",
    "    best_loss = np.Inf\n",
    "    search_iteration = 1\n",
    "    \n",
    "    gp_result = gp_minimize(func         = get_hyperopts,\n",
    "                            dimensions   = dimensions,\n",
    "                            acq_func     = 'EI', # Expected Improvement.\n",
    "                            n_calls      = 50,\n",
    "                            noise        = 0.01,\n",
    "                            n_jobs       = -1,\n",
    "                            kappa        = 5,\n",
    "                            x0           = default_parameters,\n",
    "                            random_state = SEED\n",
    "                           )\n",
    "    \n",
    "    \n",
    "    print('\\nSEARCH COMPLETE.')\n",
    "    print('MAKING VALIDATION PREDICTIONS...')\n",
    "    \n",
    "    # Load best model\n",
    "    model = load_model(model_save_name)\n",
    "    # Make validation predictions\n",
    "    preds = model.predict(X_vdx_best_model)\n",
    "    \n",
    "    # Calculate OOF loss \n",
    "    oof_loss = metric(np.array(y_vdx_best_model), np.array(preds))\n",
    "\n",
    "    print('FOLD ' + str(fold) + ' LOSS: ' + str(oof_loss))\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    time.sleep(2)\n",
    "    loss_scores.append(oof_loss)\n",
    "\n",
    "    # Clean up\n",
    "    gc.collect()\n",
    "    os.remove(model_save_name_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cba3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9888f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064fdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13c571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
