{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7992a",
   "metadata": {},
   "source": [
    "# Hotel Recognition to Combat Human Trafficking | Train\n",
    "    2021-05-09\n",
    "    Edward Sims\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.00 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf182792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import cv2\n",
    "from datetime import datetime as dt\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as k\n",
    "# Key layers\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Package options\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8795e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "# Check GPU config\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d8c86",
   "metadata": {},
   "source": [
    "## 2.00 Data Preparation\n",
    "### 2.01 Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b90807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir_path         = \"../input/hotel-id-2021-fgvc8\"\n",
    "train_images_dir_path = os.path.join(data_dir_path, \"train_images\")\n",
    "test_images_dir_path  = os.path.join(data_dir_path, \"test_images\")\n",
    "\n",
    "train_metadata_path   = os.path.join(data_dir_path, \"train.csv\")\n",
    "sample_sub_path       = os.path.join(data_dir_path, \"sample_submission.csv\")\n",
    "\n",
    "# Read csv data\n",
    "train_metadata        = pd.read_csv(train_metadata_path, parse_dates=[\"timestamp\"])\n",
    "sample_sub            = pd.read_csv(sample_sub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95040f",
   "metadata": {},
   "source": [
    "### 2.02 Set default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc440d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: resnet50_128x128_seed14\n"
     ]
    }
   ],
   "source": [
    "# Define key parameters\n",
    "SEED = 14\n",
    "np.random.seed(SEED)\n",
    "\n",
    "ROWS     = 128 # Default row size\n",
    "COLS     = 128 # Default col size\n",
    "CHANNELS = 3\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "KFOLDS = 2\n",
    "PATIENCE = 10\n",
    "\n",
    "# Read all images in and subset in CV? Or Read images inside each fold in CV?\n",
    "read_images_in_fold = True\n",
    "\n",
    "MODEL_TO_USE = \"resnet50\"\n",
    "model_name_save = MODEL_TO_USE + \"_\" + str(ROWS) + \"x\" + str(COLS) + \"_seed\" + str(SEED)\n",
    "\n",
    "# Create weights path if does not exist already\n",
    "if not os.path.exists(f\"weights/{model_name_save}\"):\n",
    "    os.mkdir(f\"weights/{model_name_save}\")\n",
    "    \n",
    "print(f\"Model name: {model_name_save}\")\n",
    "\n",
    "# Get number of cpu cores for multiprocessing\n",
    "try:\n",
    "    cpus = multiprocessing.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 1 # Default number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e37b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata preparation\n",
    "def get_is_weekend(timestamp_col):\n",
    "    \"\"\"\n",
    "    Returns boolean for whether timestamp is a weekend.\n",
    "    \"\"\"\n",
    "    timestamp_col_weekday = timestamp_col.dt.weekday\n",
    "    # Allocate booleans - Weekends are designated 6 & 7\n",
    "    timestamp_col_weekday = timestamp_col_weekday.apply(lambda x: False if x < 5 else True)\n",
    "    \n",
    "    return timestamp_col_weekday\n",
    "\n",
    "# Extract year, month and hour from timestamp feature\n",
    "train_metadata[\"year\"] = train_metadata[\"timestamp\"].dt.year\n",
    "train_metadata[\"month\"] = train_metadata[\"timestamp\"].dt.month\n",
    "train_metadata[\"hour\"] = train_metadata[\"timestamp\"].dt.hour\n",
    "# Extract is_weekend from timestamp\n",
    "train_metadata[\"is_weekend\"] = get_is_weekend(train_metadata[\"timestamp\"])\n",
    "train_metadata = train_metadata.drop(\"timestamp\", axis=1)\n",
    "\n",
    "# Extract labels from metadata\n",
    "y_train = np.array(train_metadata[\"hotel_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9fa5d",
   "metadata": {},
   "source": [
    "### 2.03 Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(image_dir_path):\n",
    "    \"\"\"Reads images into np.array from directory of image files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_dir : list\n",
    "        Directory of images to read from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of images paths to.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get list of all image paths\n",
    "    image_path_list = []\n",
    "    \n",
    "    for chain_id in os.listdir(image_dir_path):\n",
    "        # Each subdirectory is a chain_id\n",
    "        chain_id_dir_path = os.path.join(image_dir_path, chain_id)\n",
    "        # Read images from each chain_id subdirectory\n",
    "        for image in os.listdir(chain_id_dir_path): \n",
    "            # Read image\n",
    "            image_path = os.path.join(chain_id_dir_path, image)\n",
    "            # Append to list of images\n",
    "            image_path_list.append(image_path)    \n",
    "\n",
    "    return image_path_list\n",
    "\n",
    "\n",
    "def load_image(image_path, rows=ROWS, cols=COLS):\n",
    "    \"\"\"\n",
    "    Read and return resized image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (rows, cols))\n",
    "        return image\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def read_images(image_paths):\n",
    "    # Read in images asyncrously\n",
    "    pool = multiprocessing.Pool(processes=cpus)\n",
    "    image_array = np.array(pool.map(load_image, image_path_list))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3a7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all full image paths\n",
    "image_path_list = get_image_paths(train_images_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9355a2",
   "metadata": {},
   "source": [
    "### 2.04 Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe27c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgs(train_imgs, test_imgs):\n",
    "    \"\"\"Centers images by minusing the mean and dividing by std\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_imgs: array \n",
    "        Train images to read in and normalise\n",
    "    test_imgs: array\n",
    "        Test images to read in and normalise\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        2 np.arrays including original images and augmented images (for train and test set).\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Preprocessing images...\\n')\n",
    "    # Convert pixel values to float\n",
    "    train_imgs = train_imgs.astype(float)\n",
    "    test_imgs = test_imgs.astype(float)\n",
    "\n",
    "    # Get per-channel means and stds    \n",
    "    train_means = train_imgs.reshape(-1, train_imgs.shape[-1]).mean(axis=0)\n",
    "    train_stds = train_imgs.reshape(-1, train_imgs.shape[-1]).std(axis=0)\n",
    "\n",
    "    # Standardise images\n",
    "    train_imgs -= train_means\n",
    "    train_imgs /= train_stds\n",
    "    \n",
    "    test_imgs -= train_means\n",
    "    test_imgs /= train_stds\n",
    "        \n",
    "    #print(f'Train per-channel means: {train_imgs.reshape(-1, train_imgs.shape[-1]).mean(axis=0)}')\n",
    "    #print(f'Trin per-channel stds: {train_imgs.reshape(-1, train_imgs.shape[-1]).std(axis=0)}')\n",
    "\n",
    "    return train_imgs, test_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_augmentations(X_img, y, p, aug):\n",
    "    \"\"\"Make a random subset of p proportion. Apply augmentations\n",
    "        to the subset and append back to the original dataset, \n",
    "        making necessary changes to labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_img: array\n",
    "        Train images to read in and augment\n",
    "    y: array\n",
    "        Train labels to copy as per augmented images\n",
    "    p: float\n",
    "        sample size probability\n",
    "    aug: string\n",
    "        Choice of augmentation from ['fliplr', 'rot90', 'rot180', 'rot270']\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        np.array of original images and augmented images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Augmenting images...')\n",
    "    # Get a sample of X and y based on p proportion\n",
    "    sample_size = int(round(len(y) * p))\n",
    "    idx_sample = random.sample(range(0, len(y), 1), sample_size)\n",
    "\n",
    "    # Make augmentations to sample\n",
    "    if aug == 'fliplr':\n",
    "        X_img = np.concatenate(\n",
    "            (X_img, np.array([np.fliplr(X_img[i]) for i in idx_sample])),\n",
    "            axis=0\n",
    "        )\n",
    "    elif aug == 'rot90':\n",
    "        X_img = np.concatenate(\n",
    "            (X_img, np.array([np.rot90(X_img[i], 1) for i in idx_sample])),\n",
    "            axis=0\n",
    "        )\n",
    "    elif aug == 'rot180':\n",
    "        X_img = np.concatenate(\n",
    "            (X_img, np.array([np.rot90(X_img[i], 2) for i in idx_sample])),\n",
    "            axis=0\n",
    "        )\n",
    "    elif aug == 'rot270':\n",
    "        X_img = np.concatenate(\n",
    "            (X_img, np.array([np.rot90(X_img[i], 3) for i in idx_sample])),\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "    # Copy labels accordingly\n",
    "    y_sample = np.array([y[i] for i in idx_sample])\n",
    "    y = np.concatenate((y, y_sample), axis=0)\n",
    "    del y_sample\n",
    "\n",
    "    #X_img, y = shuffle(X_img, y, random_state=SEED)\n",
    "\n",
    "    return X_img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88005137",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_images_in_fold == False:\n",
    "    print(f'Train imgs shape: {X_train_img.shape}')\n",
    "print(f'Train dataframe shape: {X_train_df.shape}')\n",
    "print(f'Train targets shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674d633",
   "metadata": {},
   "source": [
    "## 3.00 Modelling\n",
    "### 3.01 Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31899b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start          = 0.00001, \n",
    "               lr_max            = 0.0008, \n",
    "               lr_min            = 0.00001, \n",
    "               lr_rampup_epochs  = 20, \n",
    "               lr_sustain_epochs = 0, \n",
    "               lr_exp_decay      = 0.8):\n",
    "    \n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "\n",
    "    return lrfn\n",
    "\n",
    "lrfn = build_lrfn()\n",
    "lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "\n",
    "plt.plot([lrfn(epoch) for epoch in range(EPOCHS)])\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8602de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df631bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df91e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07feec24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c77b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cfd04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1383800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73139d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd53a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15b58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06356c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160aba6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5529d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cba3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9888f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064fdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13c571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
